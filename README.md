Dust Particle Count Validation Dashboard :

This project was developed during my Data Analyst Internship to analyze and validate dust particle counts using YOLOv8 object detection outputs against manually annotated data. 
The goal was to compare actual (annotation) vs. predicted counts and understand the reasons for mismatches, helping improve future annotations and model accuracy.

ğŸ“Œ Project Overview

Data Source: Annotated data was downloaded from Roboflow in YOLOv8 format.
Conversion: Python script was used to extract particle counts from YOLOv8 .txt annotation files (inside a ZIP), and convert them into a structured CSV file.
Visualization: Built interactive dashboards in Power BI to compare actual vs. predicted particle counts and analyze reasons for discrepancies.

ğŸ›  Data Preparation

YOLOv8 annotations contain:
        Class ID
        Normalized x/y coordinates
        Width and height of bounding boxes
Converted annotations into CSV using Python.
Extracted:
        Image name
        Actual particle count (from annotation)
        Predicted particle count (from YOLOv8 model)

ğŸ“Š Dashboards
ğŸ”¹ 1. Comparison Dashboard

Compares manual annotation counts with YOLOv8 model predictions across 406 images.
Key Features:
        - Displays Actual vs. Predicted counts per image.
        - Highlights Maximum, Minimum, and Difference values.
Donut Chart: Categorizes accuracy into three levels:
                  âœ… Accurate (6â€“9 difference)
                  âš–ï¸ Moderately Accurate (10â€“14 difference)
                  âŒ Low Accuracy (15â€“18 difference)
        - Side-by-side comparison table for each image.
        - Slicer filters to view images by category.

ğŸ”¹ 2. Reason Dashboard

Focuses on why mismatches occur between actual and predicted counts.
Key Features:
        - Displays sample images for each accuracy category.
        - Highlights major causes of differences:
                Annotation errors
                Model limitations
                Low image quality
        - Helps identify where predictions went right or wrong.

ğŸ“ˆ Insights
        - Classified images into Accurate, Moderate, and Low Accuracy groups.
        - Identified common causes of mismatches, such as:
                Incorrect annotation drawing
                Poor-quality images
                Model struggling with overlapping particles
        - These insights will help improve future annotations and model performance.
  
